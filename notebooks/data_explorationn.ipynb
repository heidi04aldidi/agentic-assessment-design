{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6654939f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19f32d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vamshipendyala/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")  # run only once\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from text_cleaner import clean_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9006de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QUESTIONS_PATH = \"../data/raw/Questions.csv\"\n",
    "ANSWERS_PATH = \"../data/raw/Answers.csv\"\n",
    "TAGS_PATH = \"../data/raw/Tags.csv\"\n",
    "\n",
    "QUESTIONS_OUTPUT_PATH = \"../data/processed/questions_clean.csv\"\n",
    "ANSWERS_OUTPUT_PATH = \"../data/processed/answers_clean.csv\"\n",
    "\n",
    "CHUNK_SIZE = 50000  # rows processed at a time(large so chunckwise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a683f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "736c9622",
   "metadata": {},
   "source": [
    "counting questions/answers/tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question_count = sum(\n",
    "    len(chunk)\n",
    "    for chunk in pd.read_csv(QUESTIONS_PATH, encoding=\"latin1\", chunksize=100000)\n",
    ")\n",
    "\n",
    "question_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0dfb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_count = sum(\n",
    "    len(chunk)\n",
    "    for chunk in pd.read_csv(ANSWERS_PATH, encoding=\"latin1\", chunksize=100000)\n",
    ")\n",
    "\n",
    "answer_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e13e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_count = sum(\n",
    "    len(chunk)\n",
    "    for chunk in pd.read_csv(TAGS_PATH, encoding=\"latin1\", chunksize=100000)\n",
    ")\n",
    "\n",
    "tag_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad31fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_summary = pd.DataFrame({\n",
    "    \"TYpe\": [\"Questions\", \"Answers\", \"Tags\"],\n",
    "    \"Count\": [question_count, answer_count, tag_count]\n",
    "})\n",
    "\n",
    "dataset_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_ques= pd.read_csv(\n",
    "    ANSWERS_PATH,\n",
    "    encoding=\"latin1\",\n",
    "    nrows=5\n",
    ")\n",
    "\n",
    "# sample_df_ques.head()\n",
    "sample_df_ques.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72136158",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697ef3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_ques= pd.read_csv(\n",
    "    QUESTIONS_PATH,\n",
    "    encoding=\"latin1\",\n",
    "    nrows=5\n",
    ")\n",
    "\n",
    "sample_df_ques.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf70219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import XMLParsedAsHTMLWarning\n",
    "import warnings\n",
    "\n",
    "# Suppress XML warning once\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae7c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = pd.read_csv(\"../data/raw/Tags.csv\")\n",
    "print(tags_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a7a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = tags_df[\"Tag\"].unique()\n",
    "\n",
    "print(\"Total Unique Tags:\", len(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da84a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in unique_tags:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db98f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_pipeline(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Decode HTML entities (e.g., &lt; → <)\n",
    "    text = html.unescape(text)\n",
    "    try:\n",
    "        # Remove HTML/XML tags\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "\n",
    "    # Normalize multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Join back\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b15d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags_df = pd.DataFrame(unique_tags, columns=[\"Tag\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags_df.to_csv(\"../data/processed/unique_tags.csv\", index=False)\n",
    "\n",
    "print(\"Unique tags file saved successfully.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_counts = tags_df[\"Tag\"].value_counts()\n",
    "\n",
    "print(tag_counts.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66108dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cleaning on Small Subset\n",
    "test_df = pd.read_csv(\n",
    "    QUESTIONS_PATH,\n",
    "    encoding=\"latin1\",\n",
    "    nrows=10000\n",
    ")\n",
    "\n",
    "test_df[\"raw_text\"] = (\n",
    "    test_df[\"Title\"].fillna(\"\") + \" \" +\n",
    "    test_df[\"Body\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "test_df[\"clean_text\"] = test_df[\"raw_text\"].apply(clean_text_pipeline)\n",
    "\n",
    "sample = test_df[[\"raw_text\", \"clean_text\"]].iloc[0]\n",
    "\n",
    "print(\"RAW TEXT:\\n\", sample[\"raw_text\"][:500])\n",
    "print(\"\\nCLEAN TEXT:\\n\", sample[\"clean_text\"][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc6e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = pd.read_csv(\n",
    "    \"../data/raw/Questions.csv\",\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac752d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pd.read_csv(\n",
    "    \"../data/processed/unique_tags.csv\",\n",
    "    encoding=\"latin1\"\n",
    ") \n",
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52712d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tags\n",
    "tags_df = pd.read_csv(\"../data/processed/unique_tags.csv\", encoding=\"latin1\")\n",
    "\n",
    "# Convert to clean lowercase list\n",
    "tags = (\n",
    "    tags_df.iloc[:, 0]\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\"Total tags loaded:\", len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd6b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Dataset Cleaning (Chunk Processing)\n",
    "first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    QUESTIONS_PATH,\n",
    "    encoding=\"latin1\",\n",
    "    chunksize=CHUNK_SIZE\n",
    "):\n",
    "\n",
    "    # Combine title and body\n",
    "    chunk[\"raw_text\"] = (\n",
    "        chunk[\"Title\"].fillna(\"\") + \" \" +\n",
    "        chunk[\"Body\"].fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # Apply cleaning pipeline\n",
    "    chunk[\"clean_text\"] = chunk[\"raw_text\"].apply(clean_text_pipeline)\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    processed_chunk = chunk[[\"Id\", \"Score\", \"clean_text\"]]\n",
    "\n",
    "    # Write incrementally\n",
    "    processed_chunk.to_csv(\n",
    "        QUESTIONS_OUTPUT_PATH,\n",
    "        mode=\"w\" if first_chunk else \"a\",\n",
    "        header=first_chunk,\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    first_chunk = False\n",
    "\n",
    "print(\" Full preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9491164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholder mapping\n",
    "tag_placeholder_map = {}\n",
    "\n",
    "for i, tag in enumerate(tags):\n",
    "    placeholder = f\"TAGTOKEN{i}\"\n",
    "    tag_placeholder_map[tag] = placeholder\n",
    "\n",
    "# Reverse map (to restore later)\n",
    "reverse_map = {v: k for k, v in tag_placeholder_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def protect_tags(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    for tag, placeholder in tag_placeholder_map.items():\n",
    "        # replace whole word matches only\n",
    "        text = re.sub(rf\"\\b{re.escape(tag)}\\b\", placeholder, text)\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text(separator=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187227f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(os.path.dirname(QUESTIONS_OUTPUT_PATH), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal Cleaning (Without Damaging Tags)\n",
    "def clean_text(text):\n",
    "    text = remove_html(text)\n",
    "    \n",
    "    # remove special characters but keep placeholders\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)\n",
    "    \n",
    "    # remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ed1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore Tags Back\n",
    "def restore_tags(text):\n",
    "    for placeholder, tag in reverse_map.items():\n",
    "        text = text.replace(placeholder, tag)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d137241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "clean_df_sample = pd.read_csv(QUESTIONS_OUTPUT_PATH, nrows=5)\n",
    "\n",
    "clean_df_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d0b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_clean_pipeline(text):\n",
    "    #Final Pipeline Function\n",
    "    text = protect_tags(text)\n",
    "    text = clean_text(text)\n",
    "    text = restore_tags(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_count = sum(\n",
    "    len(chunk)\n",
    "    for chunk in pd.read_csv(QUESTIONS_PATH, encoding=\"latin1\", chunksize=100000)\n",
    ")\n",
    "\n",
    "clean_count = sum(\n",
    "    len(chunk)\n",
    "    for chunk in pd.read_csv(QUESTIONS_OUTPUT_PATH, chunksize=100000)\n",
    ")\n",
    "\n",
    "print(\"Original rows:\", original_count)\n",
    "print(\"Cleaned rows:\", clean_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af96b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply to DataFrame\n",
    "questions_df[\"cleaned_body\"] = questions_df[\"Body\"].apply(full_clean_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463a7d4",
   "metadata": {},
   "source": [
    "answers cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4403828",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_ans= pd.read_csv(\n",
    "    ANSWERS_PATH,\n",
    "    encoding=\"latin1\",\n",
    "    nrows=5\n",
    ")\n",
    "\n",
    "# sample_df_ans.head()\n",
    "sample_df_ques.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd008de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Cleaning on Small Subset of ans\n",
    "# test_df_ans = pd.read_csv(\n",
    "#     ANSWERS_PATH,\n",
    "#     encoding=\"latin1\",\n",
    "#     nrows=10000\n",
    "# )\n",
    "\n",
    "# test_df_ans[\"raw_text\"] = (\n",
    "#     test_df_ans[\"Body\"].fillna(\"\")\n",
    "# )\n",
    "\n",
    "# test_df_ans[\"clean_text\"] = test_df_ans[\"raw_text\"].apply(clean_text_pipeline)\n",
    "\n",
    "# sample = test_df_ans[[\"raw_text\", \"clean_text\"]].iloc[0]\n",
    "\n",
    "# print(\"RAW TEXT:\\n\", sample[\"raw_text\"][:500])\n",
    "# print(\"\\nCLEAN TEXT:\\n\", sample[\"clean_text\"][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a18138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Dataset Cleaning (Chunk Processing)\n",
    "first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    ANSWERS_PATH,\n",
    "    encoding=\"latin1\",\n",
    "    chunksize=CHUNK_SIZE\n",
    "):\n",
    "\n",
    "    # Combine title and body\n",
    "    chunk[\"raw_text\"] = (\n",
    "        chunk[\"Body\"].fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # Apply cleaning pipeline\n",
    "    chunk[\"clean_text\"] = chunk[\"raw_text\"].apply(clean_text_pipeline)\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    processed_chunk = chunk[[\"Id\",\"ParentId\", \"Score\", \"clean_text\"]]\n",
    "\n",
    "    # Write incrementally\n",
    "    processed_chunk.to_csv(\n",
    "        ANSWERS_OUTPUT_PATH,\n",
    "        mode=\"w\" if first_chunk else \"a\",\n",
    "        header=first_chunk,\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    first_chunk = False\n",
    "\n",
    "print(\" Full preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "answers_sample = pd.read_csv(\"../data/processed/answers_clean.csv\", nrows=5)\n",
    "answers_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd4c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF VECTORIZATION\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "# Load cleaned questions\n",
    "clean_questions = pd.read_csv(\"../data/processed/questions_clean.csv\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1,2),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "tfidf_questions = vectorizer.fit_transform(clean_questions[\"clean_text\"].fillna(\"\"))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(vectorizer, \"../data/processed/tfidf_vectorizer.pkl\")\n",
    "\n",
    "print(\"Questions TF-IDF Shape:\", tfidf_questions.shape)\n",
    "\n",
    "# Save sparse matrix\n",
    "save_npz(\"../data/processed/questions_tfidf.npz\", tfidf_questions)\n",
    "\n",
    "print(\"Questions TF-IDF saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the newly cleaned answers (now has ParentId)\n",
    "answers_clean = pd.read_csv(\"../data/processed/answers_clean.csv\")\n",
    "print(\"Answers shape:\", answers_clean.shape)\n",
    "print(answers_clean.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all answers by the question they belong to (ParentId = Question Id)\n",
    "answer_stats = answers_clean.groupby(\"ParentId\").agg(\n",
    "    avg_answer_score=(\"Score\", \"mean\"),\n",
    "    answer_count=(\"Score\", \"count\"),\n",
    "    max_answer_score=(\"Score\", \"max\")\n",
    ").reset_index()\n",
    "# Rename ParentId to question_id for clarity\n",
    "answer_stats.rename(columns={\"ParentId\": \"question_id\"}, inplace=True)\n",
    "print(\"Answer stats shape:\", answer_stats.shape)\n",
    "answer_stats.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4149bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998cd46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Bayesian Smoothing + Normalize to 0-1\n",
    "# (Replaces the simple min-max normalization)\n",
    "\n",
    "global_mean = answer_stats[\"avg_answer_score\"].mean()\n",
    "C = answer_stats[\"answer_count\"].mean()  # confidence factor\n",
    "\n",
    "print(f\"Global mean score: {global_mean:.2f}\")\n",
    "print(f\"Confidence factor C: {C:.2f}\")\n",
    "\n",
    "# Apply Bayesian smoothing\n",
    "answer_stats[\"bayesian_avg_score\"] = (\n",
    "    (C * global_mean + answer_stats[\"avg_answer_score\"] * answer_stats[\"answer_count\"])\n",
    "    / (C + answer_stats[\"answer_count\"])\n",
    ")\n",
    "\n",
    "# Normalize to 0-1\n",
    "min_b = answer_stats[\"bayesian_avg_score\"].min()\n",
    "max_b = answer_stats[\"bayesian_avg_score\"].max()\n",
    "\n",
    "answer_stats[\"avg_score_normalized\"] = (\n",
    "    (answer_stats[\"bayesian_avg_score\"] - min_b) / (max_b - min_b)\n",
    ")\n",
    "\n",
    "print(\"\\nNormalized score range:\")\n",
    "print(\"Min:\", answer_stats[\"avg_score_normalized\"].min())\n",
    "print(\"Max:\", answer_stats[\"avg_score_normalized\"].max())\n",
    "print(\"Mean:\", answer_stats[\"avg_score_normalized\"].mean().round(4))\n",
    "\n",
    "answer_stats.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "answer_stats[\"cluster\"] = kmeans.fit_predict(\n",
    "    answer_stats[['avg_score_normalized']]\n",
    ")\n",
    "cluster_order = (\n",
    "    answer_stats.groupby('cluster')['avg_score_normalized']\n",
    "    .mean()\n",
    "    .sort_values()\n",
    "    .index\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "    cluster_order[0]: \"Hard\",\n",
    "    cluster_order[1]: \"Medium\",\n",
    "    cluster_order[2]: \"Easy\"\n",
    "}\n",
    "\n",
    "answer_stats[\"difficulty\"] = (\n",
    "    answer_stats[\"cluster\"].map(mapping)\n",
    ")\n",
    "\n",
    "answer_stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.scatter(\n",
    "    answer_stats.index,\n",
    "    answer_stats['avg_score_normalized'],\n",
    "    c=answer_stats['cluster'],\n",
    "    cmap='viridis',\n",
    "    s=60\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Question Index\")\n",
    "plt.ylabel(\"Normalized Score (0–1)\")\n",
    "plt.title(\"K-Means Clustering of Question Difficulty\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "\n",
    "# Load cleaned answers\n",
    "clean_answers = pd.read_csv(\"../data/processed/answers_clean.csv\")\n",
    "\n",
    "# Load saved vectorizer\n",
    "vectorizer = joblib.load(\"../data/processed/tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Transform answers (DO NOT FIT AGAIN)\n",
    "tfidf_answers = vectorizer.transform(clean_answers[\"clean_text\"].fillna(\"\"))\n",
    "\n",
    "print(\"Answers TF-IDF Shape:\", tfidf_answers.shape)\n",
    "\n",
    "# Save sparse matrix\n",
    "save_npz(\"../data/processed/answers_tfidf.npz\", tfidf_answers)\n",
    "\n",
    "print(\"Answers TF-IDF saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bf256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalling the score and made it till out of 100\n",
    "\n",
    "\n",
    "# calculated the averge by group of that that particular question id.\n",
    "# clean_answers['average_score'] = (clean_answers.groupby('Id')['score_out_of_100'].transform('mean'))\n",
    "\n",
    "# # added a new column of difficulty label and hardcoded the bins such that if average score defines the difficulty.\n",
    "# def assign_difficulty(score):\n",
    "#     if score >= 70:\n",
    "#         return \"Easy\"\n",
    "#     elif score >= 40:\n",
    "#         return \"Medium\" \n",
    "#     else:\n",
    "#         return \"Hard\"\n",
    "\n",
    "# clean_answers['difficulty'] = (\n",
    "#     clean_answers['average_score']\n",
    "#     .apply(assign_difficulty)\n",
    "# )\n",
    "\n",
    "clean_answers.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
